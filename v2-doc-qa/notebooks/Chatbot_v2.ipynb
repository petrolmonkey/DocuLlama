{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ef8c19",
   "metadata": {},
   "source": [
    "# Chatbot Project v2\n",
    "In this next iteration of the project, we create a Chatbot that can answer questions about a set of documents. Basic vector search is the method behind answering questions relevant in a particular context. It entails loading documents, embedding them into a vector, then creating a vector database so an LLM can answer questions about the data. \n",
    "\n",
    "Chatbots help facilitate interaction with large language models (LLMS) such as GPT-4 or LLama through the use of API calls and user interfaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc445278-8830-47d8-ad4a-6efe9ae21303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Container Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfcdd1-97d4-4a5e-a3b5-d166a82b7ff3",
   "metadata": {},
   "source": [
    "Below are the 3 methods from the Streamlit framework to capture, view and manage messages. \n",
    "\n",
    "* `st.chat_message` used to display containers with the user's input and the bot's response.\n",
    "* `st.chat_input` a widget that allows user enter input\n",
    "* `st.session_state` a list to store the chat history so it can be displayed in the containers; a dictionary is used in the example below with keys `role` (the author of the message) and `content` (the message itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750895c6-e6e3-4562-864c-6b50ad5d5905",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment and Set Up\n",
    "Load the environment variables and create an openai client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4102d-c467-4b1d-ab3a-9d7583a31e99",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d01980-42bb-4079-9f68-5eb41e4b050f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "# Create the client (initializes API connection)\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Call the model\n",
    "completion = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a single sentence about LLMs.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118abbae-2b23-47fb-afbe-0ffb14f88671",
   "metadata": {},
   "source": [
    "# Three Steps for Direct Vector Search and Retrieval\n",
    "This section shows how documents are ingested for content extraction. They are loaded into an object for preprocessing with `SimpleDirectoryReader`.  Then the data is embedded, and stored in an index with `VectorStoreIndex`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651a80e-cc91-4f6c-b95a-e8fd8c89c124",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "LlamaIndex provides many functionalities which make it a valuable toolkit for working with LLMs. It is used here to convert PDFs into Document objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "475bd65a-761e-4df0-855f-d08e4b5934bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2265f02f-b3cc-4c80-82bf-282a9ed7fe73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the documents in the directory \"/manuals\"\n",
    "documents = SimpleDirectoryReader(\"./manuals\").load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d42178-b8e1-47d3-8fab-1f8d165e83a9",
   "metadata": {},
   "source": [
    "## Embedding the Data\n",
    "An index is a data structure made of vectors that can be queried quickly for information. `VectorStoreIndex` generates a vector representation for each chunk using an embedding model. It organizes these vectors in a database that enables fast search and retrieval. \n",
    "\n",
    "`VectorStoreIndex` can handle the parsing, chunking and embedding in one wrapper. If direct control of the process is desired, each step can be done separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174c5aca-c709-4945-95cc-97f8af2a9d19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:11:12,411 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-16 10:11:24,334 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-16 10:11:27,964 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-16 10:11:29,578 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create a vector index for all the manuals\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12eed3f-5950-41fb-b524-6a330ada74ea",
   "metadata": {},
   "source": [
    "## Create Query Engine\n",
    "\n",
    "The `as_query_engine()` method converts a question into an embedding, searches the index, and retrieves relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9056b3-c1ed-4074-b88a-7c536905d2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d4cebc-0266-45a0-ac79-2d05b6e614b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:16:09,461 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-16 10:16:10,860 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mower uses between 18 to 20 ounces of oil.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How much oil does the mower use?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8244888-5849-4f16-9576-aefae1eeb832",
   "metadata": {},
   "source": [
    "# Chatbot Integration\n",
    "Now that we can retrieve information from the documents, let us connect the indexed data to the query engine and generate responses in our chatbot. This vector search integration is plugged into the chatbot's message loop with just a couple lines of code.\n",
    "\n",
    "`NOTE` The OpenAI client is not defined because LlamaIndex manages the connection internally using the environment variable we created in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f2bb06-242b-4244-9da1-ea08ce3dbc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chatbot_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbot_v2.py\n",
    "\n",
    "import streamlit as st\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "# Streamlit decorator caches the return value of a function that produces a global \n",
    "# resource, such as a database connection or a machine learning model. Prevents the \n",
    "# resource from being re-created on every app rerun, making the app more performant. \n",
    "\n",
    "@st.cache_resource\n",
    "\n",
    "# Load and cache the index\n",
    "def load_index():\n",
    "    documents = SimpleDirectoryReader(\"./manuals\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    return index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "query_engine = load_index()\n",
    "\n",
    "# Streamlit Chat\n",
    "st.title('Instruction Manuals and Reference')\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role']):\n",
    "        st.markdown(message['content'])\n",
    "\n",
    "# User input captured: The chatbot interface captures the user's question\n",
    "if prompt := st.chat_input('Enter your question...'):\n",
    "    st.session_state.messages.append({'role':'user', 'content':prompt})\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Response generated: The LLM synthesizes an answer using the retrieved context\n",
    "    response = query_engine.query(prompt)\n",
    "    \n",
    "    st.session_state.messages.append({'role':'assistant','content':str(response)})   \n",
    "    # Response displayed: The chatbot displays the response to the user\n",
    "    with st.chat_message('assistant'):\n",
    "        st.markdown(str(response))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5b9ca-fb45-4864-9079-6ee560638a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot_v1)",
   "language": "python",
   "name": "chatbot_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
