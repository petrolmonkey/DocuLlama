{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ef8c19",
   "metadata": {},
   "source": [
    "# Chatbot v3\n",
    "With the use of LLMS, such as GPT-4 or LLama, we finally create a chatbot that can answer questions about a set of documents. When answering questions in a particular context, basic vector search is an effective methodology to use. It entails loading documents, embedding them into a vector, then creating a vector database so an LLM can answer questions about the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc445278-8830-47d8-ad4a-6efe9ae21303",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66bfcdd1-97d4-4a5e-a3b5-d166a82b7ff3",
   "metadata": {},
   "source": [
    "## Streamlit Review\n",
    "Below are the 3 methods from the Streamlit framework to capture, view and manage messages. \n",
    "\n",
    "* `st.chat_message` displays containers with the user and bot responses\n",
    "* `st.chat_input` an element that allows users to enter their questions\n",
    "* `st.session_state` stores the chat history with keys `role` and value `content`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750895c6-e6e3-4562-864c-6b50ad5d5905",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment and Set Up\n",
    "Load the environment variables and create an openai client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4102d-c467-4b1d-ab3a-9d7583a31e99",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d01980-42bb-4079-9f68-5eb41e4b050f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the api key and other variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118abbae-2b23-47fb-afbe-0ffb14f88671",
   "metadata": {},
   "source": [
    "# Three Steps for Direct Vector Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651a80e-cc91-4f6c-b95a-e8fd8c89c124",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "Documents are loaded into an object for preprocessing with `SimpleDirectoryReader`. In addition to this, LlamaIndex provides many other functionalities which make it a valuable toolkit for working with LLMs. It is used here to convert PDFs into document objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475bd65a-761e-4df0-855f-d08e4b5934bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# Load the documents in the directory \"/manuals\"\n",
    "documents = SimpleDirectoryReader(\"../manuals\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2265f02f-b3cc-4c80-82bf-282a9ed7fe73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d42178-b8e1-47d3-8fab-1f8d165e83a9",
   "metadata": {},
   "source": [
    "## Embedding the Data and Creating the Vector Database\n",
    "When text is converted into numerical vectors, data that is more readily analyzed by computers, it is said to be embedded. `VectorStoreIndex` splits the data into chunks then generates a vector representation for each using an embedding model. \n",
    "\n",
    "Once the data has been embedded, it needs to be stored in a data structure so  information can be retrieved quickly. Luckily, `VectorStoreIndex` can generate vector embeddings as well as organize them in a searchable index. The vectors are created and stored in an index with the `from_documents()` method.\n",
    "\n",
    "Although llama index provides a full pipeline, perhaps it can be customized in a future version to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174c5aca-c709-4945-95cc-97f8af2a9d19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 21:08:31,386 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 21:08:33,174 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 21:08:34,965 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 21:08:36,706 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create a vector index for all the manuals\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8244888-5849-4f16-9576-aefae1eeb832",
   "metadata": {},
   "source": [
    "# Tying it all Together\n",
    "All that is left to do is convert the indexed data to a query engine so our chatbot can start generating responses. The vector search integration is completed with just a couple lines of code in the chatbot script.\n",
    "\n",
    "`NOTE` The OpenAI client is not defined below because LlamaIndex manages the connection internally using the environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12eed3f-5950-41fb-b524-6a330ada74ea",
   "metadata": {},
   "source": [
    "## Query Engine\n",
    "The `.as_query_engine()` method converts an index object into a query engine object that can fetch relevant nodes of data. In the final code below the query engine is the return value from the `load_index()` function.\n",
    "\n",
    "When a question is input into the chatbot, it is embedded with the `.query()` method. The query engine then finds the most similar vectors in its database and provides the top 5 chunks. Below is a quick test with a simple question to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9056b3-c1ed-4074-b88a-7c536905d2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 21:08:47,371 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 21:08:48,793 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mower uses between 18 to 20 ounces of oil.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "query_engine = index.as_query_engine(similarity_top_k = 5)\n",
    "response = query_engine.query(\"How much oil does the mower use?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4cebc-0266-45a0-ac79-2d05b6e614b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance Boost\n",
    "The Streamlit decorator, `@st.cache_resource`, caches the return value of a function that produces a global \n",
    "resource; in our case, it is the index created from our documents. This prevents the index from being re-created on every app rerun, making the app more performant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f2bb06-242b-4244-9da1-ea08ce3dbc75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing v3-chatbot-doc-qa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile v3-chatbot-doc-qa.py\n",
    "\n",
    "import streamlit as st\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "# Streamlit decorator \n",
    "@st.cache_resource\n",
    "\n",
    "# Load and cache the index\n",
    "def load_index():\n",
    "    documents = SimpleDirectoryReader(\"./manuals\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    return index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "query_engine = load_index()\n",
    "\n",
    "# Streamlit Chat\n",
    "st.title('Instruction Manuals and Reference')\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role']):\n",
    "        st.markdown(message['content'])\n",
    "\n",
    "# User input captured: The chatbot interface captures the user's question\n",
    "if prompt := st.chat_input('Enter your question...'):\n",
    "    st.session_state.messages.append({'role':'user', 'content':prompt})\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Response generated: The LLM synthesizes an answer using the retrieved context\n",
    "    response = query_engine.query(prompt)\n",
    "    \n",
    "    st.session_state.messages.append({'role':'assistant','content':str(response)})   \n",
    "    # Response displayed: The chatbot displays the response to the user\n",
    "    with st.chat_message('assistant'):\n",
    "        st.markdown(str(response))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5b9ca-fb45-4864-9079-6ee560638a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot_v1)",
   "language": "python",
   "name": "chatbot_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
